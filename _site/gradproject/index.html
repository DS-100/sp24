<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/sp24/assets/css/just-the-docs-default.css"> <script src="/sp24/assets/js/vendor/lunr.min.js"></script> <script src="/sp24/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/sp24/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Graduate Project | Data 100</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Graduate Project" /> <meta name="author" content="Joseph E. Gonzalez, Narges Norouzi" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Specifications for the grad project for Data 200." /> <meta property="og:description" content="Specifications for the grad project for Data 200." /> <link rel="canonical" href="http://localhost:4000/sp24/gradproject/" /> <meta property="og:url" content="http://localhost:4000/sp24/gradproject/" /> <meta property="og:site_name" content="Data 100" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Graduate Project" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Joseph E. Gonzalez, Narges Norouzi"},"description":"Specifications for the grad project for Data 200.","headline":"Graduate Project","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/sp24/resources/assets/favicon/panda-logo.png"},"name":"Joseph E. Gonzalez, Narges Norouzi"},"url":"http://localhost:4000/sp24/gradproject/"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/sp24/" class="site-title lh-tight"> <div class="site-logo" role="img" aria-label="Data 100"></div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/sp24/" class="nav-list-link">Home / Schedule</a></li><li class="nav-list-item"><a href="/sp24/syllabus/" class="nav-list-link">Syllabus</a></li><li class="nav-list-item active"><a href="/sp24/gradproject/" class="nav-list-link active">Graduate Project</a></li><li class="nav-list-item"><a href="/sp24/calendar/" class="nav-list-link">Calendar</a></li><li class="nav-list-item"><a href="/sp24/resources/" class="nav-list-link">Resources</a></li><li class="nav-list-item"><a href="/sp24/staff/" class="nav-list-link">Staff</a></li><li class="nav-list-item"><a href="/sp24/acks/" class="nav-list-link">Acknowledgments</a></li></ul> <ul class="nav-list"><li class="nav-list-item external"> <a href="https://ds100.org/course-notes/" class="nav-list-link external"> Course Notes </a> </li><li class="nav-list-item external"> <a href="https://ds100.org/debugging-guide/" class="nav-list-link external"> Debugging Guide </a> </li><li class="nav-list-item external"> <a href="https://ds100.org/faqs/sp24/" class="nav-list-link external"> Course FAQ </a> </li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Data 100" aria-label="Search Data 100" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="no_toc" id="graduate-project">Graduate Project</h1> <!--### <span style="color:red"> ⚠️ Warning: This webpage is under construction; nothing here is finalized until it is on ds100.org/fa23/gradproject </span> {:.no_toc} --> <ul id="markdown-toc"> <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li> <li><a href="#deliverables" id="markdown-toc-deliverables">Deliverables</a> <ul> <li><a href="#upcoming-deliverables" id="markdown-toc-upcoming-deliverables">Upcoming Deliverables:</a></li> <li><a href="#future-deliverables-subject-to-change" id="markdown-toc-future-deliverables-subject-to-change">Future Deliverables (subject to change):</a></li> <li><a href="#teamwork" id="markdown-toc-teamwork">Teamwork</a></li> </ul> </li> <li><a href="#timeline-and-grading-breakdown" id="markdown-toc-timeline-and-grading-breakdown">Timeline and Grading Breakdown</a> <ul> <li><a href="#late-policy" id="markdown-toc-late-policy">Late Policy</a></li> </ul> </li> <li><a href="#datasets" id="markdown-toc-datasets">Datasets</a> <ul> <li><a href="#accessing-datasets" id="markdown-toc-accessing-datasets">Accessing Datasets</a></li> <li><a href="#topic-1-computer-vision" id="markdown-toc-topic-1-computer-vision">Topic 1: Computer Vision</a></li> <li><a href="#topic-2-natural-language-processing" id="markdown-toc-topic-2-natural-language-processing">Topic 2: Natural Language Processing</a></li> </ul> </li> <li><a href="#group-formation--research-proposal" id="markdown-toc-group-formation--research-proposal">Group Formation + Research Proposal</a></li> <li><a href="#checkpoint-1-eda--internal-peer-review" id="markdown-toc-checkpoint-1-eda--internal-peer-review">Checkpoint 1: EDA + Internal Peer Review</a></li> <li><a href="#rubrics" id="markdown-toc-rubrics">Rubrics</a> <ul> <li><a href="#group-formation--research-proposal-5" id="markdown-toc-group-formation--research-proposal-5">Group formation + Research Proposal (5%)</a></li> <li><a href="#checkpoint-1-eda--internal-peer-review-10" id="markdown-toc-checkpoint-1-eda--internal-peer-review-10">Checkpoint 1: EDA + Internal Peer Review (10%)</a></li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <p>The graduate project is <strong>offered only to students enrolled in Data C200, CS C200A, or Data 200S</strong>. Other students are welcome to explore the questions and datasets in the project for personal learning, but their work will not be graded or counted towards their final grades.</p> <p>The purpose of the project is to give students experience in both open-ended data science analysis and research in general.</p> <!-- In this project, you will work with **one or any combination** of the following datasets provided to you to explore research questions that you define. --> <!-- **Project criteria**: In addition to the general guidelines, each dataset option below has its own set of additional requirements for Report Format and Submission. Be sure to consult the correct section for your project option.) 0--> <h2 id="deliverables">Deliverables</h2> <p>The graduate project element will require the following deliverables:</p> <h3 id="upcoming-deliverables">Upcoming Deliverables:</h3> <ul> <li><strong>Group Formation + Research Proposal:</strong> You will form a project group and submit a google form stating your intended topic and a brief implementation plan. Please see <a href="#group-formation--research-proposal">below</a> for more information.</li> </ul> <h3 id="future-deliverables-subject-to-change">Future Deliverables (subject to change):</h3> <ul> <li><strong>Checkpoint 1: EDA + Internal Peer Review 1:</strong> Submit a write-up and code for Exploratory Data Analysis on your dataset. Additionally, submit an internal peer review. More information <a href="#checkpoint-1-eda--internal-peer-review">below</a>.</li> <li><strong>Checkpoint 2: Mandatory Check-In:</strong> Write a one-pager of your progress, focusing on the modeling approaches your team explored, and review it with a course staff member. Further details <a href="#checkpoint-2-mandatory-check-in">below</a>.</li> <li><strong>Project Report First Draft + Internal Peer Review 2:</strong> Submit the first draft of your report, detailing your EDA and modeling efforts, along with any necessary code. An internal peer review is also required.</li> <li><strong>External Peer-Review:</strong> Provide feedback on other project teams’ work.</li> <li><strong>Final Project Report:</strong> Submit the final project report, including all necessary code. Ensure all relevant feedback from the first draft and external peer reviews are incorporated. Additionally, you are required to make a brief 5-minute YouTube video recording of the project.</li> </ul> <h3 id="teamwork">Teamwork</h3> <p><strong>You must work in groups of two or three students.</strong> In order to give everyone experience in collaborating on a data science project, individual projects are not allowed. Everyone in the same group will receive the same grade (except for exceptional circumstances).</p> <h2 id="timeline-and-grading-breakdown">Timeline and Grading Breakdown</h2> <!-- [Internal Peer Review](https://forms.gle/cied6ZzmBToj3ARP9) --> <div class="table-wrapper"><table> <thead> <tr> <th>Deadline (at 11:59 PM Pacific)</th> <th>Event / Deliverable</th> <th>Link</th> <th>Grading Weight</th> </tr> </thead> <tbody> <tr> <td>3/15</td> <td>Research Proposal and Project Groups Due</td> <td><a href="https://forms.gle/DcBp3ZbM8TpTfSRD6" target="_blank">Proposal Form</a></td> <td>5%</td> </tr> <tr> <td>3/22 (<a href="https://edstem.org/us/courses/51810/discussion/4604306">3/24 Extension</a>)</td> <td>Checkpoint 1: EDA + Internal Peer Review 1</td> <td><a href="https://forms.gle/NgERYS9bd1U29Xur5" target="_blank">Internal Review Form</a></td> <td>10%</td> </tr> <tr> <td>Week of 4/8</td> <td>Checkpoint 2: Mandatory Check-in with TA</td> <td> </td> <td>7.5%</td> </tr> <tr> <td>4/19</td> <td>Internal Peer Review 2 Due</td> <td> </td> <td>20%</td> </tr> <tr> <td>4/26</td> <td>First Draft of Final Report Due</td> <td> </td> <td>7.5%</td> </tr> <tr> <td>5/3</td> <td>External Peer Review Due</td> <td> </td> <td>7.5%</td> </tr> <tr> <td>5/10</td> <td>Final Project Report and Presentation Video</td> <td> </td> <td>50%</td> </tr> </tbody> </table></div> <h3 id="late-policy">Late Policy</h3> <ul> <li><strong>No Extensions for First Draft</strong>: The first draft cannot be submitted late as it is crucial for the peer review process.</li> <li><strong>Final Report and Presentation Video</strong>: Late submissions incur a 10% daily penalty, up to a maximum of two days. Submissions are rounded to the nearest day (e.g., 2 minutes late counts as 1 day late).</li> <li><strong>Peer Reviews and Other Deliverables</strong>: Must be submitted on time; no extensions are permitted.</li> </ul> <h2 id="datasets">Datasets</h2> <p>This section contains the topics we will provide to you to explore your research questions. Please choose one of the following datasets to work on. <strong>You will be expected to complete all (2) tasks provided for your chosen dataset.</strong> <!-- In general, if you're drawing any conclusions regarding causality, please be sure to consult the [extra resources on causal inference](#extra-resources-causal-inference). --></p> <h3 id="accessing-datasets">Accessing Datasets</h3> <p>All of the provided datasets can be found in the Datahub directory <code class="language-plaintext highlighter-rouge">shared/sp24_grad_project_data</code>. You can access the data directly from Datahub. If you wish to work on the project locally, you can also download the files containing the datasets for each topic by right-click on the file in JupyterLab and select “Copy Download Link”. If you choose to train more complex models, DataHub might not have enough hardware resources or memory, in which case you can use <a href="https://colab.google/" target="_blank">Google Colab</a> or your local machine. If you would like to use Google Colab, feel free to check out this <a href="https://stackoverflow.com/questions/48376580/how-to-read-data-in-google-colab-from-my-google-drive" target="_blank">link</a> to get started.</p> <h3 id="topic-1-computer-vision">Topic 1: Computer Vision</h3> <p>In disaster situations, it is important for emergency response efforts to have access to quick and accurate information about an area in order to respond effectively. This project will explore how data science techniques can be useful for such efforts.</p> <h4 class="no_toc" id="project-goals">Project Goals</h4> <ul> <li>Learn to work with image data by learning to use common feature extraction techniques like Sobel edge filtering.</li> <li>Learn to work on real-world data with common complexities such as class imbalance, low signal-to-noise ratio, and high dimensional data.</li> <li>Learn how to design effective preprocessing and featurization pipelines for solving difficult machine learning tasks.</li> </ul> <h4 class="no_toc" id="mission">Mission</h4> <p>You have been hired by a crisis response agency to help assist them with your impressive data science skills! The agency has found that using satellite imagery is highly useful for supplying information for their response efforts. Unfortunately, however, annotating these high-resolution images can be a slow process for analysts. Your mission is to help address this challenge by developing an automatic computer vision approach!</p> <h4 class="no_toc" id="dataset-description">Dataset Description</h4> <p>The agency would like you to develop your approach on their internal dataset, derived from the <a href="https://xview2.org/" target="_blank">xView2 Challenge Dataset</a>. This dataset contains satellite images of buildings after various natural disasters. The buildings are labeled based on the level of damage sustained on a scale ranging from 0 (no damage) to 3 (destroyed).</p> <p>You can access all of the data within the <code class="language-plaintext highlighter-rouge">./satellite-image-data</code> directory. The dataset consists of the following folders for different natural disasters</p> <ol> <li><code class="language-plaintext highlighter-rouge">midwest-flooding</code></li> <li><code class="language-plaintext highlighter-rouge">socal-fire</code></li> <li><code class="language-plaintext highlighter-rouge">hurricane-matthew</code></li> </ol> <p>Within each folder is a zip file <code class="language-plaintext highlighter-rouge">train_images.npz</code> containing the satellite images as numpy arrays and a <code class="language-plaintext highlighter-rouge">train_labels.npy</code> file with corresponding damage level labels.</p> <blockquote> <p>Testing: In the main directory, there are also the <code class="language-plaintext highlighter-rouge">test_images_hurricane-matthew.npz</code> and <code class="language-plaintext highlighter-rouge">test_images_flooding-fire.npz</code> zip files. The first contains test images from the <code class="language-plaintext highlighter-rouge">hurricane-matthew</code> disaster and the latter consists of a combination of test images from <code class="language-plaintext highlighter-rouge">midwest-flooding</code> and <code class="language-plaintext highlighter-rouge">socal-fire</code>.</p> </blockquote> <h4 class="no_toc" id="getting-started">Getting Started</h4> <p>To help you with onboarding, the agency has provided a starter notebook <a href="https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Fsp24-student&amp;urlpath=lab%2Ftree%2Fsp24-student%2Fgrad-proj%2Fcv-satellite-images%2Fstarter.ipynb&amp;branch=main" target="_blank"><code class="language-plaintext highlighter-rouge">starter.ipynb</code></a> which will introduce you to the dataset and some useful internal tools. After completing the onboarding assignment you will be comfortable with the following:</p> <ol> <li>Loading and visualizing data using tools from <code class="language-plaintext highlighter-rouge">data_utils.py</code></li> <li>Processing different color channels in the dataset images.</li> <li>Extracting feature information from images using tools from <code class="language-plaintext highlighter-rouge">feature_utils.py</code>.</li> </ol> <h4 class="no_toc" id="exploratory-data-analysis">Exploratory Data Analysis</h4> <p>Now that you have successfully been onboarded, the agency would like you to start performing some exploratory data analysis to build an initial understanding of the data. As part of the exploratory data analysis, the agency is interested in understanding certain aspects of the dataset better. Specifically, they are looking for:</p> <ul> <li>Basic statistics about the dataset, such as the number of images per disaster type and the distribution of image sizes and damage labels.</li> <li>Insights into useful image features for classifying images based on disaster type or damage level. Previous interns have found color information to be potentially useful. You are tasked with verifying this and exploring whether color features can effectively differentiate: <ul> <li><code class="language-plaintext highlighter-rouge">midwest-flooding</code> from <code class="language-plaintext highlighter-rouge">socal-fire</code> images.</li> <li>Damage levels 1 and 3 within the <code class="language-plaintext highlighter-rouge">hurricane-matthew</code> dataset.</li> </ul> </li> </ul> <p>Please prepare an EDA report to present to the agency leadership with the above in mind.</p> <h4 class="no_toc" id="project-tasks">Project Tasks</h4> <p>Now that leadership is pleased with your initial EDA report and confident in your data science ability, they would like you to assist the agency with various tasks. <em>Please complete Task A first and then Task B.</em></p> <h4 class="no_toc" id="task-a-disaster-type-classification"><em>Task A: Disaster Type Classification</em></h4> <p>The agency consists of different subdivisions for assisting with different disaster types, e.g., fires, floods, etc. In the event of a disaster, the agency mounts its response effort by first assessing the type of disaster and then requesting the appropriate subdivision to assist with the disaster.</p> <p>Your task is to assist the agency with making this initial call quickly by automatically classifying images based on the disaster scenario. Specifically, your role will be to build a classifier that can distinguish images from the <code class="language-plaintext highlighter-rouge">midwest-flooding</code> disaster and the <code class="language-plaintext highlighter-rouge">socal-fire</code> disaster.</p> <p>To assess your performance, please submit predictions for the <code class="language-plaintext highlighter-rouge">test_images_flooding-fire.npz</code> images. This should be in a csv file <code class="language-plaintext highlighter-rouge">test_images_flooding-fire_predictions.csv</code> consisting of a single column with no header, with a 0 to indicate a <code class="language-plaintext highlighter-rouge">midwest-flooding</code> prediction and a 1 to indicate a <code class="language-plaintext highlighter-rouge">socal-fire</code> prediction. The prediction in row <em>i</em> should correspond to the <em>ith</em> image.</p> <h4 class="no_toc" id="task-b-damage-level-classification"><em>Task B: Damage Level Classification</em></h4> <p>The agency needs to know how severe a disaster is in order to allocate resources for a response effectively. The agency is especially concerned with human lives and uses building damage as an important metric for disaster severity.</p> <p>Your task is to assist the agency by automatically detecting the building damage level after a disaster. Specifically, create a damage level classifier for the <code class="language-plaintext highlighter-rouge">hurricane-matthew</code> disaster.</p> <p>To assess your performance, please submit predictions for the <code class="language-plaintext highlighter-rouge">test_images_hurricane-matthew.npz</code> images. This should be in a CSV file <code class="language-plaintext highlighter-rouge">test_images_hurricane-matthew_predictions.csv</code> consisting of a single column with no header, with a 0-3 prediction of the damage level. The prediction in row <em>i</em> should correspond to the <em>i</em>th image.</p> <h4 class="no_toc" id="resources">Resources</h4> <p>To assist you in your efforts the agency has compiled the following list of resources:</p> <ul> <li>For more background about the dataset you can look at the <a href="https://arxiv.org/pdf/1911.09296.pdf" target="_blank">paper</a> associated with the dataset.</li> <li>For image processing, <a href="https://scikit-image.org/" target="_blank">scikit-image</a> is a very useful library. This <a href="https://www.kaggle.com/code/bextuychiev/full-tutorial-on-image-processing-in-skimage" target="_blank">tutorial</a> may be helpful for learning how to use the library.</li> <li>For problems with imbalanced classes, the <a href="https://imbalanced-learn.org/stable/index.html" target="_blank">imblearn</a> library has useful tools and examples.</li> </ul> <h3 id="topic-2-natural-language-processing">Topic 2: Natural Language Processing</h3> <p>A common task in real-life data analysis involves working with text data. In this project, we will work with a dataset consisting of natural language questions asked by humans and answers provided by chatbots.</p> <h4 class="no_toc" id="project-goals-1">Project Goals</h4> <ul> <li>Prepare you to work with text data by learning common techniques like embedding generation, tokenization, and topic modeling.</li> <li>Work with real-world data in its targetted domain. The data is non-trivial in both size and complexity.</li> <li>Ask open-ended questions and answer them using data at hand.</li> </ul> <h4 class="no_toc" id="dataset-description-1">Dataset Description</h4> <p>The source dataset link is <a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations" target="_blank">here</a>. The author describes the dataset as follows:</p> <blockquote> <p>This dataset contains 33K cleaned conversations with pairwise human preferences. It is collected from 13K unique IP addresses on the Chatbot Arena from April to June 2023. Each sample includes a question ID, two model names, their full conversation text in OpenAI API JSON format, the user vote, the anonymized user ID, the detected language tag, the OpenAI moderation API tag, the additional toxic tag, and the timestamp.</p> </blockquote> <p><a href="https://chat.lmsys.org/" target="_blank">Chatbot Arena</a> is a platform where users can ask questions and two chatbots will provide answers. The user then votes on which chatbot provided the best answer. The dataset contains the questions, the answers, and the user votes.</p> <p>You can find the processed dataset in <code class="language-plaintext highlighter-rouge">./chatbot-arena-conversations.jsonl.gz</code>. The dataset is in JSON line format and compressed using gzip. It has gone through the following preprocessing steps to make analysis easier:</p> <ul> <li>Removed non-English conversations.</li> <li>Removed conversations with more than one round.</li> <li>Removed conversations classified as toxic or harmful.</li> </ul> <p>The dataset you will be working with contains <code class="language-plaintext highlighter-rouge">25322</code> rows (out of <code class="language-plaintext highlighter-rouge">33000</code> total rows) and <code class="language-plaintext highlighter-rouge">7</code> columns (<a href="https://gist.github.com/simon-mo/25c5d532bccc7f28b404cffdfe719e6e#file-example-row-json" target="_blank">example row</a>). The columns are:</p> <ul> <li><code class="language-plaintext highlighter-rouge">question_id</code>: A unique identifier for the question.</li> <li><code class="language-plaintext highlighter-rouge">model_a</code>: The name of the first chatbot model.</li> <li><code class="language-plaintext highlighter-rouge">model_b</code>: The name of the second chatbot model.</li> <li><code class="language-plaintext highlighter-rouge">winner</code>: The name of the chatbot model that won the user vote.</li> <li><code class="language-plaintext highlighter-rouge">judge</code>: The anonymized user ID that voted.</li> <li><code class="language-plaintext highlighter-rouge">conversation_a</code>: The conversation between the user and <code class="language-plaintext highlighter-rouge">model_a</code>.</li> <li><code class="language-plaintext highlighter-rouge">conversation_b</code>: The conversation between the user and <code class="language-plaintext highlighter-rouge">model_b</code>.</li> </ul> <p>There are two auxiliary datasets that you can use to help with your analysis:</p> <ul> <li><code class="language-plaintext highlighter-rouge">./chatbot-arena-prompts-embeddings.npy</code> contains the 256-dimensional text embeddings for each of the human questions. The embeddings are generated using OpenAI’s <code class="language-plaintext highlighter-rouge">text-embedding</code> model. We will explain what embeddings are and how you can use them later.</li> <li><code class="language-plaintext highlighter-rouge">./chatbot-arena-gpt3-scores.jsonl.gz</code> (<a href="https://gist.github.com/simon-mo/25c5d532bccc7f28b404cffdfe719e6e#file-example-aux-row-json" target="_blank">example row</a>) contains labels for the dataset you can use for later modeling tasks. It has the following fields: <ul> <li><code class="language-plaintext highlighter-rouge">question_id</code>: The unique identifier for the question, as seen in <code class="language-plaintext highlighter-rouge">./chatbot-arena-conversations.jsonl.gz</code>.</li> <li><code class="language-plaintext highlighter-rouge">prompt</code>: The extracted human question. This is equivalent to the first message in <code class="language-plaintext highlighter-rouge">conversation_a</code> and <code class="language-plaintext highlighter-rouge">conversation_b</code> in <code class="language-plaintext highlighter-rouge">./chatbot-arena-conversations.jsonl.gz</code>.</li> <li><code class="language-plaintext highlighter-rouge">openai_scores_raw_choices_nested</code>: The response from OpenAI GPT 3.5 model (see later for the prompt). It contains the evaluated topic model, the reason for a hardness score from 1 to 10, and the value. For each prompt, we have 3 responses from GPT 3.5 because it is a probablistic model. In fact, you will find in real world there are common multiple labels from different annotators for ground truth data. We extracted the fields into the columns below.</li> <li><code class="language-plaintext highlighter-rouge">topic_modeling_1</code>, <code class="language-plaintext highlighter-rouge">topic_modeling_2</code>, <code class="language-plaintext highlighter-rouge">topic_modeling_3</code>: The topic modeling for the first, second, and third response. Each topic should have two words.</li> <li><code class="language-plaintext highlighter-rouge">score_reason_1</code>, <code class="language-plaintext highlighter-rouge">score_reason_2</code>, <code class="language-plaintext highlighter-rouge">score_reason_3</code>: The reason for the hardness score for the first, second, and third response.</li> <li><code class="language-plaintext highlighter-rouge">score_value_1</code>, <code class="language-plaintext highlighter-rouge">score_value_2</code>, <code class="language-plaintext highlighter-rouge">score_value_3</code>: The hardness score for the first, second, and third response.</li> </ul> </li> </ul> <p>We used <a href="https://gist.github.com/simon-mo/25c5d532bccc7f28b404cffdfe719e6e#file-prompt-md" target="_blank">this prompt</a> to generate the responses. You are welcome to generate your own ground truth data. You can generate your own embeddings following <a href="https://gist.github.com/simon-mo/25c5d532bccc7f28b404cffdfe719e6e#file-using-your-own-embeddings-md" target="_blank">this</a> guide.</p> <h4 class="no_toc" id="exploratory-data-analysis-1">Exploratory Data Analysis</h4> <p>For the EDA tasks, tell us more about the data. What do you see in the data? Come up with questions and answers about them. For example, what is the win rate of GPT4? What are the most common topics? Do different judges have different preferences? What are the most common topics? What are the most common reasons for a question being hard?</p> <h4 class="no_toc" id="project-tasks-1">Project Tasks</h4> <p>Now, we aim to better understand the different chatbot models! Please complete both Task A and B. We have included example questions to consider, but you are expected to come up with your own questions to answer.</p> <p>For both task, you will be working with the data provided. But you are also expected to perform prediction on a hold out set. You can find the data in <code class="language-plaintext highlighter-rouge">arena-validation-set-prompt-only.jsonl.gz</code> in the same directory. The data contains fields <code class="language-plaintext highlighter-rouge">question_id</code>, <code class="language-plaintext highlighter-rouge">prompt</code>, <code class="language-plaintext highlighter-rouge">model_a</code>, and <code class="language-plaintext highlighter-rouge">model_b</code>. You are expected to predict the winner and the hardness score for task A and B respectively. You should submit your final prediction as a <code class="language-plaintext highlighter-rouge">csv</code> file with four columns <code class="language-plaintext highlighter-rouge">question_id</code>, <code class="language-plaintext highlighter-rouge">winner</code>, <code class="language-plaintext highlighter-rouge">hardness_score</code>. The <code class="language-plaintext highlighter-rouge">winner</code> column should be one of the four values: <code class="language-plaintext highlighter-rouge">model_a</code>, <code class="language-plaintext highlighter-rouge">model_b</code>, <code class="language-plaintext highlighter-rouge">tie</code>, or <code class="language-plaintext highlighter-rouge">tie (bothbad)</code>. The <code class="language-plaintext highlighter-rouge">hardness_score</code> should be an integer from 1 to 10.</p> <h4 class="no_toc" id="task-a-modeling-the-winning-model"><em>Task A: Modeling the Winning Model</em></h4> <p>Given a prompt, can we predict which model’s response will win the user vote? You can start by analyzing the length, textual features, and embeddings of the prompt. You should also explore the difference in output of the different models. For modeling, you can use logistic regression or other modeling techinque to perform classification to redict the winner for the hold out set given <code class="language-plaintext highlighter-rouge">prompt</code>, <code class="language-plaintext highlighter-rouge">model_a</code> and <code class="language-plaintext highlighter-rouge">model_b</code>. You should also evaluate the model using appropriate metrics.</p> <p>One hint would be to utilize topic modeling data by first clustering prompts given their embeddings, then for each cluster, train a model to predict the winner. Also, feel free to use the hardness score to help with the prediction.</p> <h4 class="no_toc" id="task-b-hardness-prediction"><em>Task B: Hardness Prediction</em></h4> <p>While we provide the hardness score generated by GPT3.5, can you explore whether such scoring is useful and valid? For hardness score, we want it to be an integer value from 1 to 10. For example, if a prompt’s score is 1, we expect the weak model to be able to answer the question. If the score is 10, we expect the question to be hard, maybe only GPT4 can answer it.</p> <p>You can start by analyzing the embeddings and the topic modeling data. You can then use linear regression to predict the hardness score, using existing or new features.</p> <p>You should also evaluate the model using appropriate metrics. One challenging aspect here is that the output score should be integer value, while linear regression is used for continuous data.</p> <h4 class="no_toc" id="getting-started-1">Getting Started</h4> <p>To get started, we provide a notebook <a href="https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Fsp24-student&amp;urlpath=lab%2Ftree%2Fsp24-student%2Fgrad-proj%2Fnlp-chatbot-arena%2Fnlp-chatbot-starter.ipynb&amp;branch=main" target="_blank"><code class="language-plaintext highlighter-rouge">nlp-chatbot-starter.ipynb</code></a> that demonstrates how to load and inspect the data.</p> <p>Additionally, here are some example questions about the project that you are welcome to explore.</p> <blockquote> <p>Modeling: Perform some modeling tasks given our ground truth labels. Can you train a logistic regression model to predict the winner given embeddings? How about a K-means clustering model to cluster the questions? Can you use linear regression to predict the hardness score? We expect you to demonstrate how the well model works and how to evaluate them. You should justify the choice of model and the evaluation metrics. You should also discuss the limitations of the model and how to improve them.</p> </blockquote> <blockquote> <p>Analysis: By leveraging the question embeddings, can we find similar questions? How repeated are the questions in the dataset? Can you reproduce the Elo score rating for the chatbots and come up with a better ranking? How can we make sense of the data overall?</p> </blockquote> <h4 class="no_toc" id="resources-1">Resources</h4> <ul> <li> <p><a href="https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH" target="_blank">Joey’s EDA and Elo rating modeling</a> is a great resource to get started with the EDA. Note that (1) the plot is made with Plotly, we recommend you to reproduce the plot with Matplotlib or Seaborn, and (2) the Elo rating is a good modeling task to reproduce but we expect you to do more than just that (for example, demonstrate how Elo rating works and how to calculate it in your report).</p> </li> <li> <p><a href="https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/" target="_blank">An intuitive introduction to text embeddings</a> is a good resource to understand what is text embeddings and how to use them.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Elo_rating_system" target="_blank">Elo rating system</a> and <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model" target="_blank">Bradley-Terry model</a> are essential to model a ranking among the pairwise comparison.</p> </li> <li> <p><a href="https://huggingface.co/docs/transformers/en/main_classes/pipelines" target="_blank">Huggingface pipeline</a> has many implementations of common NLP tasks for you to use, including sentiment analysis, summarization, text classification, etc.</p> </li> <li> <p><a href="https://spacy.io/usage/spacy-101" target="_blank">spaCy</a> is a wonderful library containing classifical NLP tasks like tokenization, lemmatization, etc.</p> </li> </ul> <h2 id="group-formation--research-proposal">Group Formation + Research Proposal</h2> <p>The first deliverable of your group project is just to form your group, choose a dataset, and submit your implementation plan to <a href="https://forms.gle/DcBp3ZbM8TpTfSRD6" target="_blank">this google form</a> by 11:59 pm on 3/15. The implementation plan should consist of a series of steps for completing the project along with a timeline. You may form groups of 2 or 3 people with any Data 200/200A/200S student.</p> <h2 id="checkpoint-1-eda--internal-peer-review">Checkpoint 1: EDA + Internal Peer Review</h2> <p>The checkpoint is intended to keep you on track to meet your project goals. You will need to submit an exploratory data analysis report to Gradescope. This will include submitting both a report of your results so far as well as all code necessary to replicate your results. Please answer all the questions below. Your submission should include:</p> <!-- - **Project Introduction and Goals:** Please briefly introduce your project. Think about introducing your project to someone who has a background in data science but does not know the dataset and your research question. This part should not exceed 500 words. Here are some components to help you get started: - What is the dataset about? How was the data collected? What are the available features and information? What is the size of the dataset? - What questions do you plan to ask about the dataset? Why do we care about such a problem? - What is your workflow for the project? Your first step, second step… - What are the models you plan to use? Why would the model be a good fit for your project? What are potential pitfalls you could run into? - What is your goal for the project? What are the expected deliverables? --> <ul> <li><strong>Data Sampling and Collection</strong> <ul> <li>How was the data read and sampled for your EDA process?</li> <li>Was there any potential bias introduced in the sampling process?</li> </ul> </li> <li><strong>Data Cleaning</strong> <ul> <li>What type of data are you currently exploring?</li> <li>What is the granularity of the data?</li> <li>What does the distribution of the data look like? Are there any outliers? Are there any missing or invalid entries?</li> <li>The data is not structured. How did you turn it into a structured format? What features have you engineered?</li> </ul> </li> <li><strong>Exploratory Data Analysis</strong> <ul> <li>Is there any correlation between the variables you are interested in exploring?</li> <li>How would you cleanly and accurately visualize the relationship among variables?</li> <li>What are your EDA questions? (For example, are there any relationships between A and B? What is the distribution of A?).</li> <li>Do you need to perform data transformations?</li> </ul> </li> <li><strong>Figures(tables, plots, etc.)</strong> <ul> <li>Descriptions of your figures. Takeaways from the figures.</li> <li>These figures must be of good quality (i.e. they must include axes, titles, labels, etc.) and they must be relevant to your proposed analysis.</li> </ul> </li> </ul> <p>Concretely, here are the minimal requirements for EDA for each project. Using your knowledge from Data 200, what would be appropriate data visualizations? You are welcome to do more than the minimal requirements.</p> <ul> <li><strong>Computer Vision</strong>: <ul> <li>Number of images per disaster.</li> <li>Image sizes in each dataset. Should ideally observe large variance in sizes, but similar distribution for each disaster.</li> <li>Damage labels. Should observe imbalances in the labels.</li> <li>(Open-ended) Visualize the distribution of color for different disasters.</li> <li>(Open-ended) Convey that the distributions are “separable” somehow.</li> </ul> </li> <li><strong>Natural Language Processing</strong>: <ul> <li>Ranking of the model based on their win rate or ELO ratings.</li> <li>Distribution of the prompt and response length.</li> <li>Hardness score distribution and its correlation with the models (e.g. GPT4 wins on hardest prompts).</li> <li>(Open-ended) Visualize the “variance” in model performance (see <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">LMSys Leaderboard</a> for an example).</li> <li>(Open-ended) Explore the prompt topics in the dataset (topic modeling).</li> </ul> </li> </ul> <p>Every member of the group also needs to submit the internal peer review <a href="https://forms.gle/NgERYS9bd1U29Xur5" target="_blank">form</a> for this checkpoint. This form is intended to record your and your group members’ progress in the project. The records will be confidential to the teaching staff and will not be shared with other students.</p> <!-- - **Other Preliminary Results (optional)**: Please optionally post any other preliminary results here for our information. --> <!-- ## Checkpoint 2: Mandatory Check-In The purpose of this checkpoint is to ensure you are making progress and on schedule to submit the first draft of the project in 2 weeks time. You will be required to make a one-page document summarizing all of your progress so far, and you will have to bring the document to a one-on-one meeting with a staff member. Please look at the <a href="#checkpoint-2-mandatory-check-in-75">rubric</a> for the checkpoint and what you need to include in the <a href="#final-project-report">Final Project Report</a> when determining what to include in your one-page document; the document should be a brief summary of all your progress so far. The staff member will quickly skim the document and give you guidance on the project as a whole. More details about submitting the one-page document and signing up for the staff member meeting will be announced on Ed soon. <!-- ## Final Project Report The project submission should include the following two components, as well as the YouTube video recording (more information to be announced later). --> <!-- ### [Component 1] Analysis Notebooks This component includes all the Jupyter Notebook(s) containing all the analyses that you performed on the datasets to support your claims in your write-up. Make sure that all references to datasets are done as `data/[path to data files]`. By running these notebooks, we should be able to replicate all the analysis/figures done in your write-up. Your analysis notebook(s) should address all of the following components in the data science lifecycle. Please note that a thorough explanation of your thought process and approach is **as important as** your work. Unreadable/uncommented code will lose points. Along with the code for the EDA portion (which also has to be included), we have provided a few additional preliminary questions/tips you can consider for the modeling portion of the project: - What are the research questions that you are answering through your analysis? What type of machine learning problem are you investigating? - Which model(s) do you use and why? - How do you use your data for training and testing? - Does your model require hyperparameter tuning? If so, how do you approach it? - How do you engineer the features for your model? What are the rationales behind selecting these features? - How do you perform cross-validation on your model? - What loss metrics are you using to evaluate your model? Why? - From a bias-variance tradeoff standpoint, how do you assess the performance of your model? How do you check if it is overfitting? - How would you improve your model based on the outcome? - Are there any further extensions to your model that would be worth exploring? ### [Component 2] Project Write-Up This is a single PDF that summarizes your workflow and what you have learned. It should be structured as a research paper and include a title, list of authors, abstract, introduction, description of data, methodology, summary of results, discussion, conclusion, and references. Make sure to number figures and tables, include informative captions, and ensure you include the provenance of the figures in the main narrative. We encourage you to render the PDF using LaTeX, but we will not be able to provide assistance with LaTeX-related issues. Specifically, you should ensure you address the following in the narrative: * Clearly state the research questions and why they are interesting and important. * Introduction: ensure you include a brief survey of related work on the topic(s) of your analysis. Be sure to reference current approaches/research in the context of your project, as well as how your project differs from or complements existing research. You must cite all the references you discuss in this section. * Description of data: ensure you outline the summary of the data and how the data was prepared for the modeling phase (summarizing your EDA work). If applicable, descriptions of additional datasets that you gathered to support your analysis may also be included. * Methodology: carefully describe the methods/models you use and why they are appropriate for answering your research questions. You must include a detailed description of how modeling is done in your project, including inference or prediction methods used, feature engineering and regularization if applicable, and cross-validation or test data as appropriate for model selection and evaluation. You may also include interesting findings involving your datasets. * Summary of results: analyze your findings in relation to your research question(s). Include/reference visualizations and specific results. Discuss any interesting findings from your analysis. You are encouraged to compare the results using different inference or prediction methods (e.g. linear regression, logistic regression, or classification and regression trees). Can you explain why some methods performed better than others? * Discussion: evaluate your approach and discuss any limitations of the methods you used. Also, briefly describe any surprising discoveries and whether there are any interesting extensions to your analysis. The narrative PDF should include figures sparingly to support specific claims. It can include a few runnable code components, but it should not have large amounts of code. The length of the report should be 8 ± 2 pages when it is printed as a PDF, excluding figures and code. Tip: if you need to write a large amount of LaTeX on markdown, you may want to use the `%%latex` cell magic. However, we also encourage you to explore [Overleaf](https://www.overleaf.com) for easily writing clean LaTeX documents. Please submit everything as a zip file to the final report submission portal on Gradescope. Please make sure the folder in the zip file has the following structure: ``` [your studentIDs joined by _]/ data/[all datasets used] analysis/[analysis notebooks] narrative/[narrative PDF] figures/[figures included in the narrative PDF] ``` Please use student IDs joined by `_` as the name for the top-level directory. The analysis notebooks must be runnable within this directory structure. If the narrative PDF includes any figures that are created in the analysis notebooks, the figures should be saved to `figures/` by the analysis notebooks. --> <h2 id="rubrics">Rubrics</h2> <p>This section includes a rubric for how different project deliverables are going to be graded. This section will be updated as we get further along the project timeline.</p> <h3 id="group-formation--research-proposal-5">Group formation + Research Proposal (5%)</h3> <ul> <li>Short paragraph description of implementation plan and timeline (2%).</li> <li>Forming teams by the deadline (3%).</li> </ul> <h3 id="checkpoint-1-eda--internal-peer-review-10">Checkpoint 1: EDA + Internal Peer Review (10%)</h3> <ul> <li>Data Sampling and Collection (0.5%).</li> <li>Data Cleaning (3%).</li> <li>Exploratory Data Analysis (3%).</li> <li>Figures (tables, plots, etc.) (3%).</li> <li>Internal Peer Review (0.5%).</li> </ul> <!-- ### Checkpoint 2: Mandatory Check-In (7.5%) - Research Questions (1.5%). - Feature Engineering (2%). - Modelling Approaches (3%). - Preliminary Results (1%). --> <!--- ### Checkpoint Rubric See the checkpoint description [here](#checkpoint). | Criterion | Points | | --- | --- | | Project Introduction and Goals | 4 | | EDA 1 | 3 | | EDA 2 | 3 | | Other Preliminary Results (optional) | 0 | ### Internal Peer Review The internal peer review is a simple google form checking if each member of the group is contributing to the project and how the tasks are distributed among members. This is graded on completion. ### External Peer Review Each group will peer review the projects from another group. The review will be graded by staff out of a total of 5 points. Each review should include the following components: 1. (1 point) A summary of the report. The summary should address at least the following: - What research question does the group propose? Why is it important? - How does the dataset relate to the research question? - What data modeling/inference techniques do the group primarily use to gain insights into their research question? Why are these techniques suitable for the task? - What are the next steps a researcher can take if they want to investigate the question further based off the work in the project? 2. (4 points, 2 per component) An evaluation of the report based on the Data Science Lifecycle. The review should include at least **one strong point and one suggestion for improvement** for each of the following components in the project: - Data collection and sampling - Data cleaning - Exploratory data analysis (data wrangling, visualization, etc.) - Data modeling (feature engineering, selection of the model, and evaluation of the model's performance, etc.) - Inference (do the results from the model sufficiently support the conclusion within the report?) The external peer review is also a great chance to learn from other people's work and reflect on the work of your own. ### Final Report: Analysis Notebook | Criterion | Points| |-------------------------------------------------------|-------| | Code readability and documentation | 5 | | Proper and sufficient utilization of Python libraries | 5 | | Overall code quality | 3 | | Replicability of the results | 7 | | **Total** | **20**| ### Final Report: Project Writeup | Criterion | Points| |------------------------------------------------------------------------|-------| | Introduction, motivation, and presentation of the research question(s) | 3 | | Exploratory data analysis | 5 | | Modeling and inference techniques | 7 | | Analysis of results | 7 | | Implementation of peer review feedback | 3 | | Discussion of potential societal impacts and/or ethical concerns | 2 | | Overall clarity and structure of the report | 3 | | **Total** | **30**| --> <!-- ## Extra Resources: Causal Inference When studying the relationship between datasets, you might want to consult the following references on causality vs. correlation. Oftentimes, it is tempting to make claims about causal relationships when there is not enough evidence from the data to support such claims. Please review the following references, or other reputable references that you find on the topic to familiarize yourself with relevant concepts and methods. * [Data 102 Data, Inference, and Decisions Spring 2020: Lecture 13: Causal Inference I. Moritz Hardt.](https://data102.org/sp20/assets/notes/notes13.pdf) * [Hernán MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) * [Advanced Data Analysis from an Elementary Point of View by Cosma Rohilla Shalizi](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/) --> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
